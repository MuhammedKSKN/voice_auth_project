import streamlit as st
import numpy as np
import librosa
import tensorflow as tf
import noisereduce as nr
import io
import soundfile as sf
import matplotlib.pyplot as plt

# --- SAYFA AYARLARI ---
st.set_page_config(
    page_title="Sesle Kimlik DoÄŸrulama & Deepfake Tespiti",
    page_icon="ğŸ™ï¸",
    layout="wide"
)

# --- BAÅLIK VE AÃ‡IKLAMA ---
st.title("ğŸ›¡ï¸ Sesle Kimlik DoÄŸrulama ve Deepfake Tespiti")
st.markdown("""
Bu uygulama, Siyam EvriÅŸimli Sinir AÄŸÄ± (Siamese CNN) kullanarak ses tabanlÄ± kimlik doÄŸrulama yapar.
Sistem, gerÃ§ek kullanÄ±cÄ± seslerini deepfake taklitlerinden ve yetkisiz kullanÄ±cÄ±lardan ayÄ±rt etmek iÃ§in tasarlanmÄ±ÅŸtÄ±r.
""")
st.markdown("---")

# --- KENAR Ã‡UBUÄU (SIDEBAR) AYARLARI ---
st.sidebar.header("âš™ï¸ Ayarlar ve Model")

# 1. EÅŸik DeÄŸeri (Threshold) AyarÄ±
# Bu deÄŸerin altÄ±nda kalan mesafeler "EÅŸleÅŸme", Ã¼stÃ¼nde kalanlar "EÅŸleÅŸmeme" sayÄ±lÄ±r.
# Modelinizi test ederken bu deÄŸeri deÄŸiÅŸtirerek en iyi noktayÄ± bulabilirsiniz.
THRESHOLD = st.sidebar.slider("Karar EÅŸik DeÄŸeri (Distance Threshold)", 0.0, 2.0, 0.5, 0.01)
st.sidebar.info(f"Mevcut EÅŸik: {THRESHOLD}. Bu deÄŸerin altÄ± 'DoÄŸrulandÄ±' kabul edilir.")

# 2. Modeli YÃ¼kleme (Ã–nbelleÄŸe alma)
@st.cache_resource
def load_siamese_model():
    # MODEL YOLUNUZU BURAYA GÄ°RÄ°N
    model_path = 'model/best_siamese_model.h5' 
    try:
        model = tf.keras.models.load_model(model_path)
        st.sidebar.success("âœ… Model baÅŸarÄ±yla yÃ¼klendi!")
        return model
    except Exception as e:
        st.sidebar.error(f"âŒ Model yÃ¼klenirken hata oluÅŸtu: {e}")
        return None

# Modeli baÅŸlat
model = load_siamese_model()

# --- YARDIMCI FONKSÄ°YONLAR (METODOLOJÄ°NÄ°ZE UYGUN) ---

def preprocess_audio_pipeline(audio_bytes, target_sr=16000, fixed_length_sec=4):
    """
    Metodolojide belirtilen 5 adÄ±mlÄ± Ã¶n iÅŸleme hattÄ±.
    Ham ses baytlarÄ±nÄ± alÄ±r, iÅŸlenmiÅŸ numpy dizisi dÃ¶ndÃ¼rÃ¼r.
    """
    try:
        # Byte verisini numpy dizisine Ã§evir
        y, sr = sf.read(io.BytesIO(audio_bytes))
        
        # 1. Format Standardizasyonu (16kHz, Mono)
        if sr != target_sr:
            y = librosa.resample(y=y, orig_sr=sr, target_sr=target_sr)
        if len(y.shape) > 1:
             y = librosa.to_mono(y)
        
        # 2. DuraÄŸan GÃ¼rÃ¼ltÃ¼ Azaltma (Stationary Noise Reduction)
        # Not: noisereduce bazen Ã§ok kÄ±sa seslerde sorun Ã§Ä±karabilir, try-except eklenebilir.
        y = nr.reduce_noise(y=y, sr=target_sr)

        # 3. Sessizlik Silme (Silence Trimming)
        y, _ = librosa.effects.trim(y, top_db=20)

        # 4. Sabit Uzunluklu Segmentasyon (Fixed-Length Segmentation - 4sn)
        target_length = int(target_sr * fixed_length_sec) # 64000 Ã¶rnek
        if len(y) < target_length:
            # Zero-padding (KÄ±sa ise sÄ±fÄ±r ekle)
            y = librosa.util.pad_center(y, size=target_length)
        else:
            # Truncation (Uzun ise kÄ±rp)
            y = y[:target_length]
            
        # 5. Normalizasyon (Genlik -1 ile 1 arasÄ±)
        y = librosa.util.normalize(y)

        return y, target_sr
    except Exception as e:
        st.error(f"Ses iÅŸleme hatasÄ±: {e}")
        return None, None

def extract_features_mfcc(processed_audio, sr=16000):
    """
    Ä°ÅŸlenmiÅŸ sesten MFCC Ã¶zelliklerini Ã§Ä±karÄ±r ve model giriÅŸine uygun ÅŸekillendirir.
    Ã‡Ä±ktÄ± Boyutu: (1, 40, 128, 1)
    """
    # Metodolojideki parametreler
    n_mfcc = 40
    n_fft = 2048
    hop_length = 512
    
    mfcc = librosa.feature.mfcc(y=processed_audio, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)
    
    # MFCC genellikle (n_mfcc, zaman) ÅŸeklindedir.
    # EÄŸer 4 saniye ise ve hop_length 512 ise zaman boyutu yaklaÅŸÄ±k 126-128 arasÄ± Ã§Ä±kar.
    # CNN giriÅŸi iÃ§in sabit boyuta (Ã¶rn: 128) emin olmak gerekebilir.
    # Burada librosa'nÄ±n Ã§Ä±ktÄ±sÄ±nÄ±n tam 128 zaman adÄ±mÄ±na denk geldiÄŸini varsayÄ±yoruz.
    # DeÄŸilse, burada da bir padding/trimming gerekebilir.
    
    # Åekillendirme: (Batch_Size, Height, Width, Channels) -> (1, 40, 128, 1)
    # Not: Zaman boyutunun 128 olduÄŸundan emin olun, deÄŸilse eÄŸitim kodunuza gÃ¶re ayarlayÄ±n.
    if mfcc.shape[1] != 128:
        mfcc = librosa.util.fix_length(mfcc, size=128, axis=1)

    mfcc_reshaped = mfcc[np.newaxis, ..., np.newaxis]
    return mfcc_reshaped, mfcc # GÃ¶rselleÅŸtirme iÃ§in ham MFCC'yi de dÃ¶ndÃ¼r

def calculate_euclidean_distance(embed1, embed2):
    """Ä°ki gÃ¶mÃ¼ vektÃ¶rÃ¼ arasÄ±ndaki Ã–klid mesafesini hesaplar."""
    # Embeddings shape: (1, 128)
    return np.linalg.norm(embed1 - embed2)

def plot_spectrogram(mfcc_data, title):
    """MFCC gÃ¶rselleÅŸtirmesi iÃ§in yardÄ±mcÄ± fonksiyon."""
    fig, ax = plt.subplots(figsize=(4, 2))
    img = librosa.display.specshow(mfcc_data, x_axis='time', ax=ax)
    fig.colorbar(img, ax=ax)
    ax.set(title=title)
    return fig

# --- ANA ARAYÃœZ ---

col1, col2 = st.columns(2)

# --- SOL KOLON: REFERANS SES (ANCHOR) ---
with col1:
    st.header("1. Referans Ses (Anchor)")
    st.write("Yetkili kullanÄ±cÄ±nÄ±n gerÃ§ek sesi.")
    
    anchor_file = st.file_uploader("Referans ses dosyasÄ± yÃ¼kle (.wav)", type=["wav", "mp3"], key="anchor")
    # Alternatif olarak mikrofondan da alÄ±nabilir ama anchor genellikle sabittir.
    
    anchor_processed = None
    anchor_features = None
    
    if anchor_file is not None:
        st.audio(anchor_file, format='audio/wav')
        with st.spinner('Referans ses iÅŸleniyor...'):
            # Byte verisini al
            anchor_bytes = anchor_file.getvalue()
            # Ã–n iÅŸleme
            anchor_processed, sr = preprocess_audio_pipeline(anchor_bytes)
            if anchor_processed is not None:
                # Ã–znitelik Ã‡Ä±karÄ±mÄ±
                anchor_features, anchor_mfcc_vis = extract_features_mfcc(anchor_processed, sr)
                st.success("Referans ses hazÄ±rlandÄ±.")
                with st.expander("SpektrogramÄ± GÃ¶ster"):
                     st.pyplot(plot_spectrogram(anchor_mfcc_vis, "Anchor MFCC"))

# --- SAÄ KOLON: TEST SESÄ° ---
with col2:
    st.header("2. Test Sesi")
    st.write("DoÄŸrulanacak ÅŸÃ¼pheli ses (Mikrofon veya Dosya).")

    # Yeni Streamlit Ã¶zelliÄŸi: Ses GiriÅŸi (Mikrofon)
    test_audio_input = st.audio_input("Mikrofon ile Kaydet", key="test_mic")
    # Veya dosya yÃ¼kleme
    test_file_upload = st.file_uploader("Veya test dosyasÄ± yÃ¼kle", type=["wav", "mp3"], key="test_file")
    
    test_file = test_audio_input if test_audio_input else test_file_upload
    
    test_processed = None
    test_features = None

    if test_file is not None:
        st.audio(test_file, format='audio/wav')
        with st.spinner('Test sesi iÅŸleniyor...'):
             # Byte verisini al
            test_bytes = test_file.getvalue()
             # Ã–n iÅŸleme
            test_processed, sr = preprocess_audio_pipeline(test_bytes)
            if test_processed is not None:
                # Ã–znitelik Ã‡Ä±karÄ±mÄ±
                test_features, test_mfcc_vis = extract_features_mfcc(test_processed, sr)
                st.success("Test sesi hazÄ±rlandÄ±.")
                with st.expander("SpektrogramÄ± GÃ¶ster"):
                     st.pyplot(plot_spectrogram(test_mfcc_vis, "Test MFCC"))

# --- DOÄRULAMA BÃ–LÃœMÃœ ---
st.markdown("---")
st.header("3. DoÄŸrulama Sonucu")

verify_button = st.button("ğŸ”Š KimliÄŸi DoÄŸrula", type="primary", use_container_width=True)

if verify_button:
    if model is None:
        st.error("Model yÃ¼klenemediÄŸi iÃ§in doÄŸrulama yapÄ±lamÄ±yor.")
    elif anchor_features is None or test_features is None:
        st.warning("LÃ¼tfen Ã¶nce hem Referans hem de Test seslerini saÄŸlayÄ±n.")
    else:
        with st.spinner('Siyam AÄŸÄ± karÅŸÄ±laÅŸtÄ±rmasÄ± yapÄ±lÄ±yor...'):
            # NOT: EÄŸittiÄŸiniz modelin Ã§Ä±ktÄ±sÄ±na gÃ¶re burasÄ± deÄŸiÅŸebilir.
            # SENARYO A: Modeliniz direkt mesafeyi (tek bir sayÄ±) dÃ¶ndÃ¼rÃ¼yorsa:
            # distance = model.predict([anchor_features, test_features])[0][0]
            
            # SENARYO B (Daha yaygÄ±n): Modeliniz iki ayrÄ± embedding dÃ¶ndÃ¼rÃ¼yorsa (Metodolojinize daha uygun):
            # Modelin iki Ã§Ä±ktÄ±sÄ± olduÄŸunu varsayÄ±yoruz: embedding_1, embedding_2
            embeddings = model.predict([anchor_features, test_features])
            embedding_anchor = embeddings[0]
            embedding_test = embeddings[1]
            
            # Ã–klid mesafesini hesapla
            distance = calculate_euclidean_distance(embedding_anchor, embedding_test)

            # --- SONUÃ‡ EKRANI ---
            st.metric(label="Hesaplanan Benzerlik Mesafesi (Ã–klid)", value=f"{distance:.4f}")
            
            if distance < THRESHOLD:
                st.success("âœ… KÄ°MLÄ°K DOÄRULANDI (Yetkili KullanÄ±cÄ±)")
                st.balloons()
            else:
                st.error("â›” KÄ°MLÄ°K REDDEDÄ°LDÄ° (Potansiyel Sahtecilik/Deepfake)")